(()=>{var ae=Object.create;var te=Object.defineProperty;var se=Object.getOwnPropertyDescriptor;var oe=Object.getOwnPropertyNames;var re=Object.getPrototypeOf,he=Object.prototype.hasOwnProperty;var le=(e,i)=>()=>(i||e((i={exports:{}}).exports,i),i.exports);var ce=(e,i,n,s)=>{if(i&&typeof i=="object"||typeof i=="function")for(let o of oe(i))!he.call(e,o)&&o!==n&&te(e,o,{get:()=>i[o],enumerable:!(s=se(i,o))||s.enumerable});return e};var ue=(e,i,n)=>(n=e!=null?ae(re(e)):{},ce(i||!e||!e.__esModule?te(n,"default",{value:e,enumerable:!0}):n,e));var ie=le((exports,module)=>{(function _f(self){"use strict";try{module&&(self=module)}catch(e){}self._factory=_f;var t;function u(e){return typeof e!="undefined"?e:!0}function aa(e){let i=Array(e);for(let n=0;n<e;n++)i[n]=v();return i}function v(){return Object.create(null)}function ba(e,i){return i.length-e.length}function x(e){return typeof e=="string"}function C(e){return typeof e=="object"}function D(e){return typeof e=="function"}function ca(e,i){var n=da;if(e&&(i&&(e=E(e,i)),this.H&&(e=E(e,this.H)),this.J&&1<e.length&&(e=E(e,this.J)),n||n==="")){if(e=e.split(n),this.filter){i=this.filter,n=e.length;let s=[];for(let o=0,r=0;o<n;o++){let h=e[o];h&&!i[h]&&(s[r++]=h)}e=s}return e}return e}let da=/[\p{Z}\p{S}\p{P}\p{C}]+/u,ea=/[\u0300-\u036f]/g;function fa(e,i){let n=Object.keys(e),s=n.length,o=[],r="",h=0;for(let l=0,f,$;l<s;l++)f=n[l],($=e[f])?(o[h++]=F(i?"(?!\\b)"+f+"(\\b|_)":f),o[h++]=$):r+=(r?"|":"")+f;return r&&(o[h++]=F(i?"(?!\\b)("+r+")(\\b|_)":"("+r+")"),o[h]=""),o}function E(e,i){for(let n=0,s=i.length;n<s&&(e=e.replace(i[n],i[n+1]),e);n+=2);return e}function F(e){return new RegExp(e,"g")}function ha(e){let i="",n="";for(let s=0,o=e.length,r;s<o;s++)(r=e[s])!==n&&(i+=n=r);return i}var ja={encode:ia,F:!1,G:""};function ia(e){return ca.call(this,(""+e).toLowerCase(),!1)}let ka={},G={};function la(e){I(e,"add"),I(e,"append"),I(e,"search"),I(e,"update"),I(e,"remove")}function I(e,i){e[i+"Async"]=function(){let n=this,s=arguments;var o=s[s.length-1];let r;return D(o)&&(r=o,delete s[s.length-1]),o=new Promise(function(h){setTimeout(function(){n.async=!0;let l=n[i].apply(n,s);n.async=!1,h(l)})}),r?(o.then(r),this):o}}function ma(e,i,n,s){let o=e.length,r=[],h,l,f=0;s&&(s=[]);for(let $=o-1;0<=$;$--){let p=e[$],y=p.length,w=v(),_=!h;for(let m=0;m<y;m++){let g=p[m],A=g.length;if(A)for(let X=0,B,k;X<A;X++)if(k=g[X],h){if(h[k]){if(!$){if(n)n--;else if(r[f++]=k,f===i)return r}($||s)&&(w[k]=1),_=!0}if(s&&(B=(l[k]||0)+1,l[k]=B,B<o)){let j=s[B-2]||(s[B-2]=[]);j[j.length]=k}}else w[k]=1}if(s)h||(l=w);else if(!_)return[];h=w}if(s)for(let $=s.length-1,p,y;0<=$;$--){p=s[$],y=p.length;for(let w=0,_;w<y;w++)if(_=p[w],!h[_]){if(n)n--;else if(r[f++]=_,f===i)return r;h[_]=1}}return r}function na(e,i){let n=v(),s=v(),o=[];for(let r=0;r<e.length;r++)n[e[r]]=1;for(let r=0,h;r<i.length;r++){h=i[r];for(let l=0,f;l<h.length;l++)f=h[l],n[f]&&!s[f]&&(s[f]=1,o[o.length]=f)}return o}function J(e){this.l=e!==!0&&e,this.cache=v(),this.h=[]}function oa(e,i,n){C(e)&&(e=e.query);let s=this.cache.get(e);return s||(s=this.search(e,i,n),this.cache.set(e,s)),s}J.prototype.set=function(e,i){if(!this.cache[e]){var n=this.h.length;for(n===this.l?delete this.cache[this.h[n-1]]:n++,--n;0<n;n--)this.h[n]=this.h[n-1];this.h[0]=e}this.cache[e]=i},J.prototype.get=function(e){let i=this.cache[e];if(this.l&&i&&(e=this.h.indexOf(e))){let n=this.h[e-1];this.h[e-1]=this.h[e],this.h[e]=n}return i};let qa={memory:{charset:"latin:extra",D:3,B:4,m:!1},performance:{D:3,B:3,s:!1,context:{depth:2,D:1}},match:{charset:"latin:extra",G:"reverse"},score:{charset:"latin:advanced",D:20,B:3,context:{depth:3,D:9}},default:{}};function ra(e,i,n,s,o,r,h){setTimeout(function(){let l=e(n?n+"."+s:s,JSON.stringify(h));l&&l.then?l.then(function(){i.export(e,i,n,o,r+1)}):i.export(e,i,n,o,r+1)})}function K(e,i){if(!(this instanceof K))return new K(e);var n;if(e){x(e)?e=qa[e]:(n=e.preset)&&(e=Object.assign({},n[n],e)),n=e.charset;var s=e.lang;x(n)&&(n.indexOf(":")===-1&&(n+=":default"),n=G[n]),x(s)&&(s=ka[s])}else e={};let o,r,h=e.context||{};if(this.encode=e.encode||n&&n.encode||ia,this.register=i||v(),this.D=o=e.resolution||9,this.G=i=n&&n.G||e.tokenize||"strict",this.depth=i==="strict"&&h.depth,this.l=u(h.bidirectional),this.s=r=u(e.optimize),this.m=u(e.fastupdate),this.B=e.minlength||1,this.C=e.boost,this.map=r?aa(o):v(),this.A=o=h.resolution||1,this.h=r?aa(o):v(),this.F=n&&n.F||e.rtl,this.H=(i=e.matcher||s&&s.H)&&fa(i,!1),this.J=(i=e.stemmer||s&&s.J)&&fa(i,!0),n=i=e.filter||s&&s.filter){n=i,s=v();for(let l=0,f=n.length;l<f;l++)s[n[l]]=1;n=s}this.filter=n,this.cache=(i=e.cache)&&new J(i)}t=K.prototype,t.append=function(e,i){return this.add(e,i,!0)},t.add=function(e,i,n,s){if(i&&(e||e===0)){if(!s&&!n&&this.register[e])return this.update(e,i);if(i=this.encode(i),s=i.length){let $=v(),p=v(),y=this.depth,w=this.D;for(let _=0;_<s;_++){let m=i[this.F?s-1-_:_];var o=m.length;if(m&&o>=this.B&&(y||!p[m])){var r=L(w,s,_),h="";switch(this.G){case"full":if(2<o){for(r=0;r<o;r++)for(var l=o;l>r;l--)if(l-r>=this.B){var f=L(w,s,_,o,r);h=m.substring(r,l),M(this,p,h,f,e,n)}break}case"reverse":if(1<o){for(l=o-1;0<l;l--)h=m[l]+h,h.length>=this.B&&M(this,p,h,L(w,s,_,o,l),e,n);h=""}case"forward":if(1<o){for(l=0;l<o;l++)h+=m[l],h.length>=this.B&&M(this,p,h,r,e,n);break}default:if(this.C&&(r=Math.min(r/this.C(i,m,_)|0,w-1)),M(this,p,m,r,e,n),y&&1<s&&_<s-1){for(o=v(),h=this.A,r=m,l=Math.min(y+1,s-_),o[r]=1,f=1;f<l;f++)if((m=i[this.F?s-1-_-f:_+f])&&m.length>=this.B&&!o[m]){o[m]=1;let g=this.l&&m>r;M(this,$,g?r:m,L(h+(s/2>h?0:1),s,_,l-1,f-1),e,n,g?m:r)}}}}}this.m||(this.register[e]=1)}}return this};function L(e,i,n,s,o){return n&&1<e?i+(s||0)<=e?n+(o||0):(e-1)/(i+(s||0))*(n+(o||0))+1|0:0}function M(e,i,n,s,o,r,h){let l=h?e.h:e.map;(!i[n]||h&&!i[n][h])&&(e.s&&(l=l[s]),h?(i=i[n]||(i[n]=v()),i[h]=1,l=l[h]||(l[h]=v())):i[n]=1,l=l[n]||(l[n]=[]),e.s||(l=l[s]||(l[s]=[])),r&&l.includes(o)||(l[l.length]=o,e.m&&(e=e.register[o]||(e.register[o]=[]),e[e.length]=l)))}t.search=function(e,i,n){n||(!i&&C(e)?(n=e,e=n.query):C(i)&&(n=i));let s=[],o,r,h=0;if(n){e=n.query||e,i=n.limit,h=n.offset||0;var l=n.context;r=n.suggest}if(e&&(e=this.encode(""+e),o=e.length,1<o)){n=v();var f=[];for(let p=0,y=0,w;p<o;p++)if((w=e[p])&&w.length>=this.B&&!n[w])if(this.s||r||this.map[w])f[y++]=w,n[w]=1;else return s;e=f,o=e.length}if(!o)return s;i||(i=100),l=this.depth&&1<o&&l!==!1,n=0;let $;l?($=e[0],n=1):1<o&&e.sort(ba);for(let p,y;n<o;n++){if(y=e[n],l?(p=sa(this,s,r,i,h,o===2,y,$),r&&p===!1&&s.length||($=y)):p=sa(this,s,r,i,h,o===1,y),p)return p;if(r&&n===o-1){if(f=s.length,!f){if(l){l=0,n=-1;continue}return s}if(f===1)return ta(s[0],i,h)}}return ma(s,i,h,r)};function sa(e,i,n,s,o,r,h,l){let f=[],$=l?e.h:e.map;if(e.s||($=ua($,h,l,e.l)),$){let p=0,y=Math.min($.length,l?e.A:e.D);for(let w=0,_=0,m,g;w<y&&!((m=$[w])&&(e.s&&(m=ua(m,h,l,e.l)),o&&m&&r&&(g=m.length,g<=o?(o-=g,m=null):(m=m.slice(o),o=0)),m&&(f[p++]=m,r&&(_+=m.length,_>=s))));w++);if(p){if(r)return ta(f,s,0);i[i.length]=f;return}}return!n&&f}function ta(e,i,n){return e=e.length===1?e[0]:[].concat.apply([],e),n||e.length>i?e.slice(n,n+i):e}function ua(e,i,n,s){return n?(s=s&&i>n,e=(e=e[s?i:n])&&e[s?n:i]):e=e[i],e}t.contain=function(e){return!!this.register[e]},t.update=function(e,i){return this.remove(e).add(e,i)},t.remove=function(e,i){let n=this.register[e];if(n){if(this.m)for(let s=0,o;s<n.length;s++)o=n[s],o.splice(o.indexOf(e),1);else N(this.map,e,this.D,this.s),this.depth&&N(this.h,e,this.A,this.s);if(i||delete this.register[e],this.cache){i=this.cache;for(let s=0,o,r;s<i.h.length;s++)r=i.h[s],o=i.cache[r],o.includes(e)&&(i.h.splice(s--,1),delete i.cache[r])}}return this};function N(e,i,n,s,o){let r=0;if(e.constructor===Array)if(o)i=e.indexOf(i),i!==-1?1<e.length&&(e.splice(i,1),r++):r++;else{o=Math.min(e.length,n);for(let h=0,l;h<o;h++)(l=e[h])&&(r=N(l,i,n,s,o),s||r||delete e[h])}else for(let h in e)(r=N(e[h],i,n,s,o))||delete e[h];return r}t.searchCache=oa,t.export=function(e,i,n,s,o){let r,h;switch(o||(o=0)){case 0:if(r="reg",this.m){h=v();for(let l in this.register)h[l]=1}else h=this.register;break;case 1:r="cfg",h={doc:0,opt:this.s?1:0};break;case 2:r="map",h=this.map;break;case 3:r="ctx",h=this.h;break;default:return}return ra(e,i||this,n,r,s,o,h),!0},t.import=function(e,i){if(i)switch(x(i)&&(i=JSON.parse(i)),e){case"cfg":this.s=!!i.opt;break;case"reg":this.m=!1,this.register=i;break;case"map":this.map=i;break;case"ctx":this.h=i}},la(K.prototype);function va(e){e=e.data;var i=self._index;let n=e.args;var s=e.task;switch(s){case"init":s=e.options||{},e=e.factory,i=s.encode,s.cache=!1,i&&i.indexOf("function")===0&&(s.encode=Function("return "+i)()),e?(Function("return "+e)()(self),self._index=new self.FlexSearch.Index(s),delete self.FlexSearch):self._index=new K(s);break;default:e=e.id,i=i[s].apply(i,n),postMessage(s==="search"?{id:e,msg:i}:{id:e})}}let wa=0;function O(e){if(!(this instanceof O))return new O(e);var i;e?D(i=e.encode)&&(e.encode=i.toString()):e={},(i=(self||window)._factory)&&(i=i.toString());let n=typeof window=="undefined"&&self.exports,s=this;this.o=xa(i,n,e.worker),this.h=v(),this.o&&(n?this.o.on("message",function(o){s.h[o.id](o.msg),delete s.h[o.id]}):this.o.onmessage=function(o){o=o.data,s.h[o.id](o.msg),delete s.h[o.id]},this.o.postMessage({task:"init",factory:i,options:e}))}P("add"),P("append"),P("search"),P("update"),P("remove");function P(e){O.prototype[e]=O.prototype[e+"Async"]=function(){let i=this,n=[].slice.call(arguments);var s=n[n.length-1];let o;return D(s)&&(o=s,n.splice(n.length-1,1)),s=new Promise(function(r){setTimeout(function(){i.h[++wa]=r,i.o.postMessage({task:e,id:wa,args:n})})}),o?(s.then(o),this):s}}function xa(a,b,c){let d;try{d=b?eval('new (require("worker_threads")["Worker"])("../dist/node/node.js")'):a?new Worker(URL.createObjectURL(new Blob(["onmessage="+va.toString()],{type:"text/javascript"}))):new Worker(x(c)?c:"worker/worker.js",{type:"module"})}catch(e){}return d}function Q(e){if(!(this instanceof Q))return new Q(e);var i=e.document||e.doc||e,n;this.K=[],this.h=[],this.A=[],this.register=v(),this.key=(n=i.key||i.id)&&S(n,this.A)||"id",this.m=u(e.fastupdate),this.C=(n=i.store)&&n!==!0&&[],this.store=n&&v(),this.I=(n=i.tag)&&S(n,this.A),this.l=n&&v(),this.cache=(n=e.cache)&&new J(n),e.cache=!1,this.o=e.worker,this.async=!1,n=v();let s=i.index||i.field||i;x(s)&&(s=[s]);for(let o=0,r,h;o<s.length;o++)r=s[o],x(r)||(h=r,r=r.field),h=C(h)?Object.assign({},e,h):e,this.o&&(n[r]=new O(h),n[r].o||(this.o=!1)),this.o||(n[r]=new K(h,this.register)),this.K[o]=S(r,this.A),this.h[o]=r;if(this.C)for(e=i.store,x(e)&&(e=[e]),i=0;i<e.length;i++)this.C[i]=S(e[i],this.A);this.index=n}function S(e,i){let n=e.split(":"),s=0;for(let o=0;o<n.length;o++)e=n[o],0<=e.indexOf("[]")&&(e=e.substring(0,e.length-2))&&(i[s]=!0),e&&(n[s++]=e);return s<n.length&&(n.length=s),1<s?n:n[0]}function T(e,i){if(x(i))e=e[i];else for(let n=0;e&&n<i.length;n++)e=e[i[n]];return e}function U(e,i,n,s,o){if(e=e[o],s===n.length-1)i[o]=e;else if(e)if(e.constructor===Array)for(i=i[o]=Array(e.length),o=0;o<e.length;o++)U(e,i,n,s,o);else i=i[o]||(i[o]=v()),o=n[++s],U(e,i,n,s,o)}function V(e,i,n,s,o,r,h,l){if(e=e[h])if(s===i.length-1){if(e.constructor===Array){if(n[s]){for(i=0;i<e.length;i++)o.add(r,e[i],!0,!0);return}e=e.join(" ")}o.add(r,e,l,!0)}else if(e.constructor===Array)for(h=0;h<e.length;h++)V(e,i,n,s,o,r,h,l);else h=i[++s],V(e,i,n,s,o,r,h,l)}t=Q.prototype,t.add=function(e,i,n){if(C(e)&&(i=e,e=T(i,this.key)),i&&(e||e===0)){if(!n&&this.register[e])return this.update(e,i);for(let s=0,o,r;s<this.h.length;s++)r=this.h[s],o=this.K[s],x(o)&&(o=[o]),V(i,o,this.A,0,this.index[r],e,o[0],n);if(this.I){let s=T(i,this.I),o=v();x(s)&&(s=[s]);for(let r=0,h,l;r<s.length;r++)if(h=s[r],!o[h]&&(o[h]=1,l=this.l[h]||(this.l[h]=[]),!n||!l.includes(e))&&(l[l.length]=e,this.m)){let f=this.register[e]||(this.register[e]=[]);f[f.length]=l}}if(this.store&&(!n||!this.store[e])){let s;if(this.C){s=v();for(let o=0,r;o<this.C.length;o++)r=this.C[o],x(r)?s[r]=i[r]:U(i,s,r,0,r[0])}this.store[e]=s||i}}return this},t.append=function(e,i){return this.add(e,i,!0)},t.update=function(e,i){return this.remove(e).add(e,i)},t.remove=function(e){if(C(e)&&(e=T(e,this.key)),this.register[e]){for(var i=0;i<this.h.length&&(this.index[this.h[i]].remove(e,!this.o),!this.m);i++);if(this.I&&!this.m)for(let n in this.l){i=this.l[n];let s=i.indexOf(e);s!==-1&&(1<i.length?i.splice(s,1):delete this.l[n])}this.store&&delete this.store[e],delete this.register[e]}return this},t.search=function(e,i,n,s){n||(!i&&C(e)?(n=e,e=""):C(i)&&(n=i,i=0));let o=[],r=[],h,l,f,$,p,y,w=0;if(n)if(n.constructor===Array)f=n,n=null;else{if(e=n.query||e,f=(h=n.pluck)||n.index||n.field,$=n.tag,l=this.store&&n.enrich,p=n.bool==="and",i=n.limit||i||100,y=n.offset||0,$&&(x($)&&($=[$]),!e)){for(let m=0,g;m<$.length;m++)(g=ya.call(this,$[m],i,y,l))&&(o[o.length]=g,w++);return w?o:[]}x(f)&&(f=[f])}f||(f=this.h),p=p&&(1<f.length||$&&1<$.length);let _=!s&&(this.o||this.async)&&[];for(let m=0,g,A,X;m<f.length;m++){let B;if(A=f[m],x(A)||(B=A,A=B.field,e=B.query||e,i=B.limit||i),_)_[m]=this.index[A].searchAsync(e,i,B||n);else{if(s?g=s[m]:g=this.index[A].search(e,i,B||n),X=g&&g.length,$&&X){let k=[],j=0;p&&(k[0]=[g]);for(let H=0,ee,R;H<$.length;H++)ee=$[H],(X=(R=this.l[ee])&&R.length)&&(j++,k[k.length]=p?[R]:R);j&&(g=p?ma(k,i||100,y||0):na(g,k),X=g.length)}if(X)r[w]=A,o[w++]=g;else if(p)return[]}}if(_){let m=this;return new Promise(function(g){Promise.all(_).then(function(A){g(m.search(e,i,n,A))})})}if(!w)return[];if(h&&(!l||!this.store))return o[0];for(let m=0,g;m<r.length;m++){if(g=o[m],g.length&&l&&(g=za.call(this,g)),h)return g;o[m]={field:r[m],result:g}}return o};function ya(e,i,n,s){let o=this.l[e],r=o&&o.length-n;if(r&&0<r)return(r>i||n)&&(o=o.slice(n,n+i)),s&&(o=za.call(this,o)),{tag:e,result:o}}function za(e){let i=Array(e.length);for(let n=0,s;n<e.length;n++)s=e[n],i[n]={id:s,doc:this.store[s]};return i}t.contain=function(e){return!!this.register[e]},t.get=function(e){return this.store[e]},t.set=function(e,i){return this.store[e]=i,this},t.searchCache=oa,t.export=function(e,i,n,s,o){if(o||(o=0),s||(s=0),s<this.h.length){let r=this.h[s],h=this.index[r];i=this,setTimeout(function(){h.export(e,i,o?r:"",s,o++)||(s++,o=1,i.export(e,i,r,s,o))})}else{let r,h;switch(o){case 1:r="tag",h=this.l;break;case 2:r="store",h=this.store;break;default:return}ra(e,this,n,r,s,o,h)}},t.import=function(e,i){if(i)switch(x(i)&&(i=JSON.parse(i)),e){case"tag":this.l=i;break;case"reg":this.m=!1,this.register=i;for(let s=0,o;s<this.h.length;s++)o=this.index[this.h[s]],o.register=i,o.m=!1;break;case"store":this.store=i;break;default:e=e.split(".");let n=e[0];e=e[1],n&&e&&this.index[n].import(e,i)}},la(Q.prototype);var Ba={encode:Aa,F:!1,G:""};let Ca=[F("[\xE0\xE1\xE2\xE3\xE4\xE5]"),"a",F("[\xE8\xE9\xEA\xEB]"),"e",F("[\xEC\xED\xEE\xEF]"),"i",F("[\xF2\xF3\xF4\xF5\xF6\u0151]"),"o",F("[\xF9\xFA\xFB\xFC\u0171]"),"u",F("[\xFD\u0177\xFF]"),"y",F("\xF1"),"n",F("[\xE7c]"),"k",F("\xDF"),"s",F(" & ")," and "];function Aa(e){var i=e=""+e;return i.normalize&&(i=i.normalize("NFD").replace(ea,"")),ca.call(this,i.toLowerCase(),!e.normalize&&Ca)}var Ea={encode:Da,F:!1,G:"strict"};let Fa=/[^a-z0-9]+/,Ga={b:"p",v:"f",w:"f",z:"s",x:"s",\u00DF:"s",d:"t",n:"m",c:"k",g:"k",j:"k",q:"k",i:"e",y:"e",u:"o"};function Da(e){e=Aa.call(this,e).join(" ");let i=[];if(e){let n=e.split(Fa),s=n.length;for(let o=0,r,h=0;o<s;o++)if((e=n[o])&&(!this.filter||!this.filter[e])){r=e[0];let l=Ga[r]||r,f=l;for(let $=1;$<e.length;$++){r=e[$];let p=Ga[r]||r;p&&p!==f&&(l+=p,f=p)}i[h++]=l}}return i}var Ia={encode:Ha,F:!1,G:""};let Ja=[F("ae"),"a",F("oe"),"o",F("sh"),"s",F("th"),"t",F("ph"),"f",F("pf"),"f",F("(?![aeo])h(?![aeo])"),"",F("(?!^[aeo])h(?!^[aeo])"),""];function Ha(e,i){return e&&(e=Da.call(this,e).join(" "),2<e.length&&(e=E(e,Ja)),i||(1<e.length&&(e=ha(e)),e&&(e=e.split(" ")))),e||[]}var La={encode:Ka,F:!1,G:""};let Ma=F("(?!\\b)[aeo]");function Ka(e){return e&&(e=Ha.call(this,e,!0),1<e.length&&(e=e.replace(Ma,"")),1<e.length&&(e=ha(e)),e&&(e=e.split(" "))),e||[]}G["latin:default"]=ja,G["latin:simple"]=Ba,G["latin:balance"]=Ea,G["latin:advanced"]=Ia,G["latin:extra"]=La;let W=self,Y,Z={Index:K,Document:Q,Worker:O,registerCharset:function(e,i){G[e]=i},registerLanguage:function(e,i){ka[e]=i}};(Y=W.define)&&Y.amd?Y([],function(){return Z}):W.exports?W.exports=Z:W.FlexSearch=Z})(exports)});var ne=ue(ie());var z=document.getElementById("search__text"),q=document.getElementById("search__suggestions");z!==null&&document.addEventListener("keydown",e=>{e.ctrlKey&&e.key==="/"?(e.preventDefault(),z.focus()):e.key==="Escape"&&(z.blur(),q.classList.add("search__suggestions--hidden"))});document.addEventListener("click",e=>{q.contains(e.target)||q.classList.add("search__suggestions--hidden")});document.addEventListener("keydown",e=>{if(q.classList.contains("search__suggestions--hidden"))return;let n=[...q.querySelectorAll("a")];if(n.length===0)return;let s=n.indexOf(document.activeElement);if(e.key==="ArrowDown"){e.preventDefault();let o=s+1<n.length?s+1:s;n[o].focus()}else e.key==="ArrowUp"&&(e.preventDefault(),nextIndex=s>0?s-1:0,n[nextIndex].focus())});(function(){let e=new ne.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/blog/20230827-a-simple-but-powerful-lemma-in-drawing-balls/",title:"A Simple but Powerful Lemma in Drawing balls from bags",description:"This blog is about a lemma I found useful in calculating probability in ball-drawing problems.",content:`Consider such a problem that bag A has $a$ white balls and $b$ black balls and bag B has $\\alpha$ white balls and $\\beta$ black balls. Now draw $c(c\\leq a+b)$ balls from the bag A to the bag B. Then what is the probability that drawing a white ball from the bag B?
To solve this problem, one can directly calculate the probability using the law of total probability. In the calculation, one will find an interesting fact which I summarize as a lemma below.
Consider a bag of $N$ balls where some are white and some are black. If the number of white balls is a random variable $X$, the probability of drawing a white ball is $\\mathbb{E}[X]/N$.
Simple and intuitive! The proof for this lemma is direct if using law of total probability. Now let&rsquo;s reconsider the problem. Note that the expected number of white balls drew from bag A is $ca/(a+b)$. Thus the required probability is $(\\alpha + ca/(a+b))/(\\alpha+\\beta+c)$. This is just a one line calculation!
Now let&rsquo;s consider a more complicated problem. We still consider the bag A and B but now we draw one ball from each bag and exchange them. After $n$ draws, what is the probability that drawing a white ball from the bag A?
Obviously, we are unsure about the number of white balls in the bag A after $n$ draws. Then let&rsquo;s say it is a random variables $X_n$ and similarly $Y_n$ is the number of white balls in bag B after $n$ draws. Thus we have a relationship between them
$$ X_n + Y_n = a + \\alpha. $$ Note that if we let $\\xi_n$ and $\\eta_n$ be the number of white balls drew from the bag A and B in the $n$th draw, we have $$ X_n = X_{n-1} + \\eta_n - \\xi_n. $$ Since $\\xi_n$ and $\\eta_n$ can only be 0 or 1, the probability that they equals to 1 has the same value as their expectations. This is where our lemma can play a role! $\\rightarrow\\ \\mathbb{E}[\\xi_n] = \\mathbb{E}[X_{n-1}]/(a+b)$ and $\\mathbb{E}[\\eta_n] = \\mathbb{E}[Y_{n-1}]/(\\alpha+\\beta)$. Then one can derive the final recursion equation, solving which will give the final answer $$ \\mathbb{E}[X_n] = \\mathbb{E}[X_{n-1}] + \\frac{a+\\alpha-\\mathbb{E}[X_{n-1}]}{\\alpha+\\beta} - \\frac{\\mathbb{E}[X_{n-1}]}{a+b}. $$
Use this lemma in your probability problem set and it will guarantee very fast computation!
`}).add({id:1,href:"/blog/20230827-when-uncertainty-meets-uncertainty/",title:"When Uncertainty Meets Uncertainty",description:`In this blog, I would like to make a comparison between Binomial and Beta-binomial distribution to understand how the uncertainty in parameters affects probability distribution. I need to point out the Bayesian approach can not completely bypass the existance of parameters (or there will be no more statisticians in this world!) and it may even bring more. But it can provide us with an intuition about how the uncertainty in parameters affects the original distribution.
Video. (1) `,content:`In this blog, I would like to make a comparison between Binomial and Beta-binomial distribution to understand how the uncertainty in parameters affects probability distribution. I need to point out the Bayesian approach can not completely bypass the existance of parameters (or there will be no more statisticians in this world!) and it may even bring more. But it can provide us with an intuition about how the uncertainty in parameters affects the original distribution.
Video. (1) Consider a general parametric distribution $X\\sim F(x|\\theta)$ and the priori of the parameters $\\theta\\sim \\pi(\\theta|\\alpha)$. Then by simple calculation, we can cancel out $\\theta$ and derive another distribution $F(x|\\alpha)=\\int F(x|\\theta)\\pi(\\theta|\\alpha)d\\theta$. For example, if we consider a binomial distribution with $p\\sim\\beta(\\alpha,\\beta)$, its resulting marginal distribution is beta-binomial distribution, which has pdf $$ \\Pr(X=k) = {n \\choose k }\\frac{\\beta(x+\\alpha,n-x+\\beta)}{\\beta(\\alpha,\\beta)}. $$ Then the uncertainty of the parameter $p$ is described by another two parameters $\\alpha$ and $\\beta$. To understand how the uncertainty of $p$ affects the original binomial distribution, we can do a simple simulation. First, note that the mean of beta-binomial distribution is $n\\alpha/(\\alpha+\\beta)$ and to fix the mean of both beta-binomial and binomial, we change $\\alpha$ and $\\beta$ with the same rate and let $p=\\alpha/(\\alpha+\\beta)$. Video. (1) indicates that as $\\alpha$ and $\\beta$ increase, the variance of $p$ decrease and thus the beta-binomial is more similar to binomial. Also, we can find that beta-binomial is more flat than blinomial, which means that we are less likely to assign high probability to any point. It aligns well with our intuition. This can also be reflected by the variance of two distribution, where binomial has $np(1-p)$ and beta-binomial has $np(1-p)(\\alpha+\\beta+n)/(\\alpha+\\beta+1)$. The extra $(\\alpha+\\beta+n)/(\\alpha+\\beta+1)$ is the description of additional uncertainty brought by $p$. Similarly, we can fix one of $\\alpha$ and $\\beta$ and see what will happen.
Video. (2) Video. (3) From the above two videos, we can also see how $\\alpha$ and $\\beta$ controls the shape of beta distribution.
We can also do analysis for Gamma-Poisson and Gamma-Gaussian distribution but the approach is similar. The code generating the video in this blog can be found at this repo.
`}).add({id:2,href:"/blog/20230801-rcplex-installation/",title:"Rcplex Installation",description:"In this blog, I will introduce how to install Rcplex on Windows.",content:`I need to install Rcplex recently to replicate the results of one paper. However, the direct use of install.packages(&ldquo;Rcplex&rdquo;) did not work on Windows. In this article, I will introduce how to successfully install Rcplex on your Windows.
Step 1 Get your cplex source file on https://www.ibm.com/products/ilog-cplex-optimization-studio. Let&rsquo;s say you install the Cplex Studio at D:/Cplex Studio/;
Step 2 Download the Rcplex .gz package on https://cran.r-project.org/web/packages/Rcplex/index.html and unzip it to D:/Rcplex/;
Step 3 Open the src/Makevars.win and modify it to
CPLEX_DIR="D:/Cplex Studio/cplex" ifeq "$(WIN)" "64" PKG_LIBS = -L"\${CPLEX_DIR}/bin/x64_win64" -lcplex1263 -lm PKG_CPPFLAGS = -D_LP64 -I"\${CPLEX_DIR}/include" -DBUILD_CPXSTATIC else PKG_LIBS = -L"\${CPLEX_DIR}/bin/x86_win32" -lcplex1263 -lm PKG_CPPFLAGS = -I"\${CPLEX_DIR}/include" endif Step 4 Open the src/Rcplex_QCP.c and comment the lines 4-8. (Because the variables are defined in another .c file.)
// The actual solving procedure is called using the function Rcplex #include "Rcplex.h" //CPXENVptr env; //CPXLPptr lp; //int numcalls; //int max_numcalls; //int forceCplxClose; Step 5 Run the following commands on your terminal.
R CMD build --no-build-vignettes --no-manual --md5 D:/Rcplex R CMD INSTALL --build --no-multiarch D:/Rcplex_{version}.tar.gz Step 6 Now theoretically you should have Rcplex successfully installed. Check with library(Rcplex)! (It may need you to install slam first.)
`}).add({id:3,href:"/blog/20230627-collections-math/",title:"Collections of Interesting Mathsmatical Problems",description:"This blog is used to collect some hard but interesting math problem I have encountered when I am preparing quant interview.",content:`Unsolved:
Analysis - Calculus - 3, 13 Brain Teaser# $\\mathbb{Q}^+$ can not be listed ascendingly. (the mean of two rational number is still a rational number.) Every function can be written as sum of an odd function and an even function. Find a one-to-one mapping between $\\mathbb{Z}$ and $\\mathbb{Z}^+$. ($2k$ and $2k+1$) $0.99999\\cdots&lt; 1$? (The uniqueness of limits) Which one is larger, $e^{\\pi}$ or $\\pi^e$? $i^i$ (What is the value of $\\ln(i)$? Use Euler formula.) Gambler A has (n+1) fair coins and gambler B has n fair coins. $\\mathbb{P}(\\text{A has more heads than B if both flip all their coins})=?$ (sysmetricity!) Given $N$ points drawn randomly on the circumference of a circle, what is the probability that they are all within a semicircle? (starting from an arbitrary point $i$, what is the probability that the other $N-1$ points lies on the corresponding clockwise semicircle?) What is the 100th digit to the right of the decimal point in $(1+\\sqrt{2})^{3000}$? (calculate the value of $(1+\\sqrt{2})^{3000}$ + $(1-\\sqrt{2})^{3000}$ using binomial expansion twice.) One amoeba can have 4 equiprobable states: die, stay the same, split into two or split into three. Starting from one amoeba, what is the probability that we have no amoeba in the end? (view the population generated by two amoebas as two population with the same probability of dying out.) A and B are playing a game alternatively: in each round, they all have probability $p$ of losing. What is the probability that the one playing the game first first lose the game? (use conditional probability!) Analysis# Calculus# $\\lim (a^n + b^n)^{1/n}=\\max(a,b)$ (can be generalize) $\\lim \\sqrt{n}(\\sqrt{n+1} - \\sqrt{n-1})=1$ $a_0=a, a_1=b, a_n=1/2(a_{n-1}+a_{n-2})$, then $\\lim a_n=1/3(a+2b)$ (can not use fixed point trick. Then how to solve?) $\\mathbb{R}$ is uncountable. (use Cantor theorem. If $[0,1]$ is countable and we have a sequence of bounded closed interval, what will happen?) In $\\mathbb{R}$, there does not exists an uncountable collection of mutually disjoint intervals. (Find a rational number within each intervals.) $(a,b)$ can not be written as a countable union of disjoint closed interval. (use Bolzano theorem.) $\\lim \\sin(\\sin(\\cdots \\sin(x))) = 0$ (monotone sequence + continuity) Let $f$ be a continuous and decreasing function on $\\mathbb{R}$. Then $f\\circ f$ has either infinite number of fixed points or odd number of fixed points. Is there any continuous function $f$ satisfying $f\\circ f = e^{-x}$? (consider monotonity.) Is there any continuous function $f$ satisfying $f\\circ f = e^{x}$? (Yes!) Prove that the Riemann function is nowhere differentiable. (consider the decimal representation of rational number.) Bounded and convex function must be constant. If $a_{n+1}=\\arctan a_n, a_1\\in\\mathbb{R}, \\lim na_n^2=?$ `}).add({id:4,href:"/blog/20230626-think-deeper-odd-coin/",title:"Think Again - Odd Coin (Defective Ball) Problem",description:"Given a set of 12 balls , one of which is defective (it weighs either less or more) . You are allow to weigh 3 times to find the defective and also tell which weighs less or more.",content:` Given a set of 12 balls , one of which is defective (it weighs either less or more) . You are allow to weigh 3 times to find the defective and also tell which weighs less or more.
The above is a question on A Practical Guid to Quantitative Finance Interview. The book gives a more general answer to this book without any explanation &ndash; you can identify the defective ball among up to $(3^n - 3) / 2$ balls using no more than $n$ measurements. To understand how to derive this formula, first we need to formulate an algorithm to solve the defective ball problem. This author provides a very beautiful idea.
We first consider a simple case i.e. we know whether the defective ball is heavier or lighter in advance. In this case, we can identify the defective ball among up to $3^n$ balls using no more than $n$ measurements. Why? Because we can first split the balls into 3 groups with the same number of balls and then place two of them on the pans of the balance. The result of the weight can help us reduce the problem size by $2/3$. Thus we only need to consider the exactly same type of questions of size $3^n,3^{n-1},\\cdots,3,1$ in sequence.
Now we try to reduce the original problem to the above simplified one. To achieve this, we need to reduce the bag of balls to the one with identifier &ldquo;heavier&rdquo; or &ldquo;lighter&rdquo;. The author proposed a rotation method. In this article, we only consider its general form. Suppose that now we have $(3^n - 3) / 2$ balls. We first divide the balls into 3 groups of $(3^{n-1} - 1) / 2$ balls, labeled as A,B and C respectively. Next, we divide the balls within each group into $n-1$ bags of size $1/3/3^2/\\cdots/3^{n-2}$. We weigh A and B on the balance and place C on the table. This is the first weighing. Then we rotate the bags of $3^{n-2}$ balls, moving the one from the right pan to the table, the one from the left pan to the right pan and the one from the table onto the left pan. If the condition of the balance changes, it indicates which bags the defective ball belongs to and whether it is heavier or lighter. (Why? Consider a simple case and write down the transition table.) Then the problem is reduced to the simple case of $3^{n-2}$ balls which requires $n-2$ measurements. So in total, we need $n$ measurements. If the condition doesn&rsquo;t change, we rotate the bags of $3^{n-3}$ balls and repeat the actions above. We can prove that this process always requires $n$ measurements since if the condition of the balance changes when rotating the bags of $3^{n-k}$ balls, we need $k + n - k$ measurements.
`}).add({id:5,href:"/blog/20221030-jordan-hahn-thm/",title:"An intuitive understanding of the proof for Jordan-Hahn decomposition",description:"In this article, I will introduce the proof for Jordan-Hahn decomposition from an intuitive perspective.",content:`Jordan-Hahn decomposition is one of the most important theorem in measure theory but its proof is both lengthy and hard to understand, at least for beginners. In this post, I want to give an intuitive understanding on its proof to illustrate why it constructs a set like that and what it is doing.
Jordan-Hahn decomposition states as follows,
Let $\\nu$ be a signed measure on $(\\Omega, \\mathcal{F})$. Then it has a decomposition $$\\nu = \\nu^+ - \\nu^- $$ where $$ \\nu^+(A) = \\sup \\{\\nu(B)|B\\subset A, B \\in \\mathcal{F}\\} $$$$ \\nu^-(A) = \\sup \\{-\\nu(B)|B\\subset A, B \\in \\mathcal{F}\\} $$ and both $\\nu^+$ and $\\nu^-$ are measure and one of them is a finite measure. Furthermore, there exists a set $D\\in \\mathcal{F}$ s.t. $$ \\nu^+(A) = \\nu(A\\cap D),\\ \\nu^-(A) = \\nu(A\\cap D^c) $$
Before we move on to its proof, let&rsquo;s consider a special case - indefinite integration. Let $f$ be a measurable function on $(\\Omega, \\mathcal{F}, \\mu)$ and define the $\\nu$ as follows $$ \\nu(A) = \\int_{A} f d\\mu $$ Clearly, it is a signed measure and it has a natural decomposition $$ \\nu(A) = \\int_{A} f^+ d\\mu - \\int_{A} f^- d\\mu $$ Now let&rsquo;s check whether the definition of $\\nu^+$ and $\\nu^-$ in the Jordan-Hahn theorem can be applied to this case. Consider an arbitrary $A\\in \\mathcal{F}$ and $\\nu^+(A)$. Clearly, the following figure illustrates that $\\nu^+(A) = \\int_{A} f^+ d\\mu$ in this case, where $\\nu(B) = \\nu^+(A)$. So we may assume that there may be an &ldquo;underlying $f$&rdquo; for every signed measure $\\nu$ (In fact, this is not true by Lebesgue decomposition). What we need to do now is to find the $f^+$ without mentioning $f$, and to find the $f^+$, we just need to find its &ldquo;support&rdquo; i.e. those sets in $\\mathcal{F}$ where $f^+$ not equal to zero on every point of them since if so, we have $f(x) = f^+(x)$ or $\\nu(A) = \\nu^+(A)$. So how to find these &ldquo;support&rdquo; and how to find the biggest one of them?
Now we only have one tool to do this - measure (or integration). Note that in the indefinite integration case, we have if $\\forall B \\subset A\\in \\mathcal{F}$, $B\\in \\mathcal{F}$, $\\int_B fd\\mu \\geq 0$, then $f \\geq 0$ on $A$. Here we can translate the integration to measure i.e. if $\\forall B \\subset A\\in \\mathcal{F}$, $B\\in \\mathcal{F}$, $\\nu(B) \\geq 0$, then $f \\geq 0$. The picture is clear now. We just need to find those set s.t. every subset of it has positive measure. So we define $$ \\mathcal{B} = \\{B\\in \\mathcal{F} | \\forall C\\in \\mathcal{F}, C\\subset B, \\nu(C) \\geq 0\\} = \\{B\\in \\mathcal{F} | \\nu^-(B) = 0\\} $$ We can be confident to say that this $\\mathcal{B}$ captures all part of &ldquo;$f^+$&rdquo;. Our next task is to find the real support of &ldquo;$f^+$&rdquo; i.e. a $N\\in\\mathcal{B}$ s.t. $f^+ = 0$ on $N^c$. There are two way to do it: (i) find the set with the biggest measure; (ii) find the biggest set. One can refer to Meature Theory by Dr. Jiaan Yan to see how to do the first one. Here I will use the Zorn Lemma to go through the second one. Define the partial order on $\\mathcal{B}$ to be $\\subset$. Since $\\mathcal{B}$ is closed under union (why?), by Zorn Lemma, we can find the biggest set $D$. So we have answered the two questions mentioned above.
Our last task is to show that $\\nu^+(A) = \\nu(A\\cap D)$, which is intuitive based on our discussion above. Before moving on, we need to test whether the $D^c$ capture the support of $f^-$. Intuitively, if there is a $B \\subset D^c$ s.t. $\\nu(B) &gt; 0$, we can show that $\\nu^- (B) &gt; 0$. If not, there will be a contradiction to our construction of $D$, which can be illustrated by the following figure. where the original $D$ and $B$ will constitute a larger $D$. Thus we have $\\nu(B) &gt; 0$ with $\\nu^- (B) &gt; 0$ now, which is clearly a contradiction when $\\nu$ is an indefinite integral. For general cases, this case is like the following case. So we need to detract the $B_1$ part of $B$ and get the contradition we want, because, intuitively, the remaining $B_2$ has $\\nu(B) &gt; 0$ with $\\nu^- (B) = 0$. A contradition! In fact, we can find a subset of $B$, say $C_1$, satisfies $-\\nu (C_1) &gt; 0$ by the definition of $\\nu^-$. This means that $B - C_1 \\subset D^c$ with $\\nu (B - C_1) &gt; 0$ and $\\nu^- (B - C_1) &gt; 0$. By induction, we can find a series of disjoint sets $\\{C_i\\}_{i\\geq 1}$ s.t. $$ \\nu(B) = \\nu(B - \\sum_{i=1}^\\infty C_i) + \\sum_{i=1}^\\infty \\nu(C_i) $$ This shows that $\\sum_{i=1}^\\infty \\nu(C_i) &gt; -\\infty$. Thus $\\nu(C_i) \\rightarrow 0$ and it implies $\\nu^{-} (B - \\sum_{i=1}^\\infty C_i) = 0$ $\\rightarrow B - \\nu(\\sum_{i=1}^\\infty C_i) \\leq 0$. This contradicts to $B - \\sum_{i=1}^\\infty C_i \\subset D^c$.
So far, we have shown that $\\nu \\geq 0$ on $D$ and $\\nu \\leq 0$ on $D^c$. The final step, $\\nu^+(A) = \\nu(A\\cap D)$, can be directly proved by the definition of $\\nu^+$ and $\\nu^-$. Thus we completes the proof of the John-Hahn decomposition.
`}).add({id:6,href:"/blog/20221008-jackknife-intro/",title:"Why does Jackknife method works?",description:"In this article, I will introduce why jackknife can be used to do debiasing and variance estimation and why it works well. Specifically, I will focus on its statistical properties e.g. consistency and biasedness.",content:`In the Advanced Mathematical Statistics course in Spring AY21/22, we studied two popular model-free computational methods used in estimation: Jackknife and Bootstrap. Given $n$ observations $X_1,\\cdots, X_n$ and a statistics $T_n$ as the estimation for unknown parameter $\\theta$ (e.g. mean value and variance), the Jackknife method for bias estimation formulates as follows, $$ b_{JACK} = (n - 1)(\\bar{T}_{n-1} - T_n) $$ where $\\bar{T}_{n-1} = n^{-1}\\sum_{i=1}^n T_{n-1,i} = n^{-1}\\sum_{i=1}^n T_{n-1}(X_1,\\cdots,X_{i-1},X_{i+1},\\cdots,X_n)$ represents the estimator generated by $n-1$ observations in our full dataset. This result was beautiful, since we don&rsquo;t need any additional information even when the true bias of the estimator contains unknow parameter $\\theta$. But there is one question: is this simple bias estimator accurate?
This formula was first proposed by Quenouille in 1949 and the motivation was that suppose the bias $b$ has the expansion1 (why?) $$ b = \\frac{a_1}{n} + \\frac{a_2}{n^2} + \\cdots $$ Then we can verify that by subtracting $b_{JACK}$ from the original $T$, we can reduce the estimation bias from $O(n^{-1})$ to $O(n^{-2})$. In fact, if we consider the case where $T_n=g(\\bar{X}_n)$ is used to estimate $\\theta=g(\\mu)$ and $g$ is sufficiently smooth2(e.g. method of moment), we have Taylor expansion $$ T_n - \\theta = g^{\\prime}(\\mu) (\\bar{X}_n -\\mu) + \\frac{1}{2}(\\bar{X}_n -\\mu)^Tg^{\\prime\\prime}(\\mu)(\\bar{X}_n -\\mu) + R_n $$ where $R_n=O_{p}(n^{-2})$3 and $$ E[g^{\\prime}(\\mu) (\\bar{X}_n -\\mu)] = 0 $$ $$ E[(\\bar{X}_n -\\mu)^Tg^{\\prime\\prime}(\\mu)(\\bar{X}_n -\\mu)] = O(n^{-1}) $$ Thus in this case the expansion $(2)$ is correct.
Let&rsquo;s further calculate the $b_{JACK}$ in this case. Aagain by Taylor expansion, we have $$ T_{n-1, i} - T_n = g^{\\prime}(\\bar{X}_n)(\\bar{X}_{n-1,i} - \\bar{X}_n) + \\frac{1}{2}(\\bar{X}_{n-1,i} - \\bar{X}_n)^Tg^{\\prime\\prime}(\\xi_i)(\\bar{X}_{n-1,i} - \\bar{X}_n) $$
Then $$ b_{JACK} = (n - 1)(\\bar{T}_{n-1} - T_n) = \\frac{n-1}{2n}\\sum_{i=1}^n (\\bar{X}_{n-1,i} - \\bar{X}_n)^Tg^{\\prime\\prime}(\\xi_i)(\\bar{X}_{n-1,i} - \\bar{X}_n) $$ $$ = \\frac{1}{2n(n-1)}\\sum_{i=1}^n (X_i - \\bar{X}_n)^Tg^{\\prime\\prime}(\\xi_i)(X_i - \\bar{X}_n) $$
where $\\bar{X}_{n-1,i}$ is the mean value of $n-1$ observations without the ith one. Since $\\xi_i \\rightarrow \\mu $ as $n \\rightarrow \\infty$, we can show that $b_{JACK} \\rightarrow E[(\\bar{X}_n -\\mu)^Tg^{\\prime\\prime}(\\mu)(\\bar{X}_n -\\mu)]$. By $E[g^{\\prime}(\\mu) (\\bar{X}_n -\\mu)] = 0$, we actually prove that the jackknife bias estimator is a consistent estimator of the first two order of the true bias.
From the above discussion, jackknife bias estimation works well for most estimators i.e. estimators have the form $g(\\bar{X}_n)$ as $n\\rightarrow \\infty$. In fact, for a more general class of statistics named functional statistics4 (e.g. quantile), we have similar result in the consistency of jackknife bias estimation.
Jackknife method is popular for its variance estimation, which was first proposed by Tukey in 1958. In fact, the name &ldquo;jackknife&rdquo; was first proposed by Tukey because, like a physical jack-knife (a compact folding knife), it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool5. By doing jackknife bias reduction, i.e. subtracting bias estimation from the original estimator $T_n$, we have $$ T_{JACK} = T_n - (n-1)(\\bar{T}_{n-1} - T_n) = \\frac{1}{n}\\sum_{i=1}^n [nT_n - (n-1)T_{n-1,i}] $$ This form is informative since if we set $\\tilde{T}_{n,i} = nT_n - (n-1)T_{n-1,i}$, named jackknife pseudovalues, $T_{JACK}$ is the mean value of some new statistics $\\tilde{T}_{n,i}$&rsquo;s. Tukey conjectured that $\\tilde{T}_{n,i}$&rsquo;s can be treated as though they were i.i.d. This conjecture was verified by Thorburn in 19776, who proved that if pseudovalues converge in mean square error to some random variables, these random variables are independent. Note that if we view $T_{JACK}\\approx T_n$, $var(T_n)$, if exists, should approximately equal to $var(\\tilde{T}_{n,i})/n$. This was actually the second conjecture proposed by Tukey. With this conjecture, Tukey defined the jackknife variance estimator as follows, $$ v_{JACK} = \\frac{1}{n(n-1)}\\sum_{i=1}^n (\\tilde{T}_{n,i} - \\frac{1}{n}\\sum_{i=1}^n \\tilde{T}_{n,i})^2 $$
Now let&rsquo;s consider some simple examples. If $T_n = \\bar{X}_n$, $T_n$ is unbiased estimator for $\\mu$. Then $$ \\bar{T}_{n-1} = \\frac{1}{n(n-1)}\\sum_{i=1}^n (nT_n - X_i) = T_n $$ Thus $b_{JACK} = 0$. Further, $v_{JACK}$ is the ordinary variance estimator for $\\bar{X}_n$ with bias correction. In this case, the jackknife and the traditional provide the same estimators.
If we consider a more complicated one2 where $T_n = \\bar{X}_n^2$ as the estimator of $\\mu^2$. It can be shown that $E[T_n] = \\mu^2 + \\sigma^2/n$ and bias$(T_n) = \\sigma^2/n$. We expect that in this case, jackknife bias reduction is powerful since bias$(T_n)$ has the form as $(2)$ and $T_n$ is a MM estimator. In fact, the jackknife bias estimation is the same as the plug-in method i.e. replace $\\sigma^2$ with its sample form. A straightforward calculation shows that $$ var(T_n) = \\frac{4\\mu^2\\sigma^2}{n} + \\frac{4\\mu\\alpha_3}{n^2} + \\frac{\\alpha_4}{n^3} $$ where $\\alpha_k$ denotes the kth central moment. The jackknife variance estimator is $$ v_{JACK} = \\frac{4\\bar{X}_n^2\\hat{\\sigma}^2}{n} - \\frac{4\\bar{X}_n\\hat{\\alpha}_3}{n(n-1)} + \\frac{\\hat{\\alpha}_4}{n(n-1)^2} - \\frac{\\hat{\\sigma}^4}{n^2(n-1)} $$ where $\\hat{\\alpha}_k$ are the kth sample central moment with bias correction i.e. the denominator is $n - 1$. We notice that $(10)$ and $(11)$ are asymptotically the same and $(10)$ is complicated. This case shows the power and value of jackknife method: we don&rsquo;t need to derive the complicated explicit form of variance like $(10)$. We just need to calculate $(8)$. For statistics like sample quantile, $\\alpha$-trimmed sample mean, and some more complicated ones like V-statistics and U-statistics in robust statistics, we can simply use the jackknife method to estimate its bias and variance.
Now, similar to our discussion in jackknife bias estimation, we need to consider the correctness of jackknife variance estimator. For jackknife variance estimator, we can show that (A) for some classes of statistics, the jackknife variance estimator is consistent; (B) the jackknife variance estimator is almost unbiased or positively biased. We first consider the property (A). Let $T_n = g(\\bar{X}_n)$ where g is sufficiently smooth (in fact, we only require it is continuously differentiable). If $\\nabla g (\\mu) \\neq 0$, we have $$ \\frac{v_{JACK}}{\\sigma_n^2} \\rightarrow 1\\ \\ \\ a.s. $$ where $\\sigma_n$ is the asymptotic variance of $T_n$ (its explicit form can be given by the Delta&rsquo;s method). This indicates that $v_{JACK}$ can be viewed as a variance estimator given by the Delta&rsquo;s method through some simple calculations. In fact, the proof of this relation2 mainly approximate the $T_n$ by some linear statistics (that&rsquo;s why we assume g is sufficiently smooth with $\\nabla g (\\mu) \\neq 0$), which shares the same idea with the Delta&rsquo;s method.
Now let&rsquo;s consider the property (B). The relevant result was first obtained by Efron in 19817. The main part of the proof is the ANOVA decomposition on estimators. Assume that $T_n$, an estimator for $\\theta$, satisfies $ET_n^2 &lt; \\infty$. Then we have $$ T_n = \\alpha^{(0)} + \\frac{1}{n}\\sum_{i} \\alpha_i^{(1)} + \\frac{1}{n^2} \\sum_{i_1 &lt; i_2} \\alpha_{i_1i_2}^{(2)} + \\cdots + \\frac{1}{n^n} \\alpha_{12\\cdots n}^{(n)} $$
where $$\\alpha^{(0)} = ET_n $$ $$\\alpha_{i}^{(1)} = n[E(T_n|X_i) - \\mu] $$ $$\\alpha_{i_1i_2}^{(2)} = n^2[E(T_n|X_{i_1}, X_{i_2}) - E(T_n|X_{i_1}) - E(T_n|X_{i_2}) + \\mu] $$ $$\\cdots $$ $$\\alpha_{i_1i_2\\cdots i_k}^{(k)} = n^k[E(T_n|X_{i_1}, X_{i_2}, \\cdots, X_{i_k}) - \\sum_{s=1}^k E(T_n|(X_{i_j})_{j\\neq s}) + $$ $$\\ \\ \\sum_{s,t=1}^k E(T_n|(X_{i_j})_{j\\neq s,t}) + \\cdots + (-1)^k\\mu $$ and these terms have zero expectation and mutually uncorrelated. This decomposition is powerful since it doesn&rsquo;t assume independence on the original observations. Intuitively, it decomposes an estimator into the main effect, mutual effect and high order effect of observations. One just need to plug in the definition of $\\alpha^{(i)}$ into the decomposition to verfify it. Notice that if we assume our observations are i.i.d., we have $$ var(T_{n-1}) = \\frac{1}{n-1}\\sigma^2_1 + {n-1 \\choose 2}\\frac{1}{(n-1)^4}\\sigma^2_2 + \\cdots + \\frac{1}{n^{2n}}\\sigma_n^2 $$ where $\\sigma_i^2 = var(\\alpha^{(i)})$. Calculating the expectation of the jackknife variance estimator using $(13)$ and comparing it with $(15)$, one can show that $$ Ev_{JACK} \\geq var(T_{n-1}) $$ This shows that the &ldquo;bias&rdquo; of the jackknife variance estimator is positive (note that the RHS of the inequality is $T_{n-1}$ not $T_{n}$). For statistics like U-statistics, von Mises Series, e.t.c., we can replace $var(T_{n-1})$ with $var(T_{n})$.
So far, we have discussed the consistency of the jackknife method and the bias of the jackknife variance estimator. These properties guarantee the jackknife method to be a good method. In fact, our discussion can be extended to functional statistics and delete-d jackknife method (which can be used to estimate the sampling distribution of the $T_n$ like bootstrap but less computationally complex). One can refer to The Jackknife and Bootstrap by Shao, J. and Tu, D..
Efron, B., 1982. The jackknife, the bootstrap and the other resampling plans.&#160;&#x21a9;&#xfe0e;
Shao, J. and Tu, D., 1995. The Jackknife and Bootstrap.&#160;&#x21a9;&#xfe0e;&#160;&#x21a9;&#xfe0e;&#160;&#x21a9;&#xfe0e;
$O_p(n^{-k})$ here means $n^kO_p(n^{-k})$ is bounded in probability.&#160;&#x21a9;&#xfe0e;
If the true parameter has the form $T(F)$ where $F$ is the population distribution, its estimator can be obtained easily by plug-in method i.e. replace $F$ with empirical distribution $F_n$. We call this statistics functional statistics.&#160;&#x21a9;&#xfe0e;
Jackknife Resampling&#160;&#x21a9;&#xfe0e;
Thorburn, D. E., 1977. On the asymptotic normality of the jackknife, Scand. J. Statist., 4, 113-118&#160;&#x21a9;&#xfe0e;
Efron, B. and Stein, C., 1981. The Jackknife Estimate of Variance. The Annals of Statistics, 9(3).&#160;&#x21a9;&#xfe0e;
`}).add({id:7,href:"/blog/20220929-mistake-measurable-functions/",title:"A Mistake on Composition of Measurable Function",description:"This article is about a mistake I made on composition of measurable function.",content:`In abstract measure theory, we have one proposition about the composition of measurable function. Suppose $(\\Omega_1,\\Sigma_1)$, $(\\Omega_2,\\Sigma_2)$ and $(\\Omega_3,\\Sigma_3)$ are measurale spaces, and $f:(\\Omega_1,\\Sigma_1) \\rightarrow (\\Omega_2,\\Sigma_2)$ and $g:(\\Omega_2,\\Sigma_2) \\rightarrow (\\Omega_3,\\Sigma_3)$ are measurable functions. Then $g\\circ f$ is a measurable function from $(\\Omega_1,\\Sigma_1)$ to $(\\Omega_3,\\Sigma_3)$.
The proof of this proposition is trivial. But one thing confused me: in Lebesgue measure theory, we have a proposition that composition of a real-valued continuous function and a real-valued measurable function is measurable but the opposite (composition of a measurable function and a continuous function is measurable) is not right. The counterexample can be constructed using Cantor function and Vitali Set. This seems to contradict the above proposition since continuous functions are measurable. Where is the mistake?
Let&rsquo;s review theory on Lebesgue measure to find the mistake. The definition of Lebesgue measurable function is
Given a measurable space $(\\Omega, \\Sigma)$ and a topological space $(U,\\tau)$. A function $f$ is measurable if $\\forall A \\in \\tau$, $f^{-1}(A) \\in \\Sigma$.
In $\\mathbb{R}$, its topology is the collection of all open interval. Thus the above definition is the same as using Boreal field to define measurable functions from $(\\Omega, \\Sigma)$ to $(\\mathbb{R}, \\mathcal{B(\\mathbb{R})})$. Lebesgue measurable function (one dimension) means the domain of the measurable function is Lebesgue measurable space $(\\mathbb{R}, \\mathcal{L})$. Obviously, a continuous function on $\\mathbb{R}$ is a Lebesgue measurable function. Let&rsquo;s make it clearer here. Let $f$ be the Lebesgue measurable function and $g$ be the continuous function. Then
$$f:(\\mathbb{R}, \\mathcal{L}) \\rightarrow (\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$$
$$g:(\\mathbb{R}, \\mathcal{L}) \\rightarrow (\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$$
The mistake is clear now. The $(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$ and $(\\mathbb{R}, \\mathcal{L})$ are not the same thing! This does not satisfy the premise of the proposition! Since $\\mathcal{L}$ is a complete $\\sigma$-field, it can hide something nasty into any zero measure set. Then the &ldquo;normal&rdquo; $\\mathcal{B}(\\mathbb{R})$ makes it reappear in front of us.
This mistake confuse me for about 30 minutes. I think next time I should write down every measurable space I use to avoid problems like this.
`}),z.addEventListener("input",function(){let n=this.value,s=e.search(n,5,{enrich:!0}),o=new Map;for(let r of s.flatMap(h=>h.result))o.has(r.href)||o.set(r.doc.href,r.doc);if(q.innerHTML="",q.classList.remove("search__suggestions--hidden"),o.size===0&&n){let r=document.createElement("div");r.innerHTML=`No results for "<strong>${n}</strong>"`,r.classList.add("search__no-results"),q.appendChild(r);return}for(let[r,h]of o){let l=document.createElement("a");l.href=r,l.classList.add("search__suggestion-item"),q.appendChild(l);let f=document.createElement("div");f.textContent=h.title,f.classList.add("search__suggestion-title"),l.appendChild(f);let $=document.createElement("div");if($.textContent=h.description,$.classList.add("search__suggestion-description"),l.appendChild($),q.childElementCount===5)break}})})();})();
//! Source: https://github.com/h-enk/doks/blob/master/assets/js/index.js
/*! Source: https://dev.to/shubhamprakash/trap-focus-using-javascript-6a3 */
//! Source: https://discourse.gohugo.io/t/range-length-or-last-element/3803/2
